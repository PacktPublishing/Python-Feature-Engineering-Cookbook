{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling to vector unit  length / unit norm\n",
    "\n",
    "Scaling to unit norm is achieved by dividing each feature vector by either the Manhattan distance (l1 norm) or the Euclidean distance of the vector (l2 norm):\n",
    "\n",
    "X_scaled_l1 = X / l1(X)\n",
    "\n",
    "X_scaled_l2 = X / l2(X)\n",
    "\n",
    "\n",
    "The **Manhattan distance** is given by the sum of the absolute components of the vector:\n",
    "\n",
    "l1(X) = |x1| + |x2| + ... + |xn|\n",
    "\n",
    "\n",
    "Whereas the **Euclidean distance** is given by the square root of the square sum of the component of the vector:\n",
    "\n",
    "l2(X) = sqr( x1^2 + x2^2 + ... + xn^2 )\n",
    "\n",
    "\n",
    "In the above example, x1 is variable 1, x2 variable 2, and xn variable n, and X is the data for 1 observation across variables (a row in other words).\n",
    "\n",
    "\n",
    "### Scaling to unit norm, examples\n",
    "\n",
    "For example, if our data has 1 observations (1 row) and 3 variables:\n",
    "\n",
    "- number of pets\n",
    "- number of children\n",
    "- age\n",
    "\n",
    "The values for each variable for that single observation are 10, 15 and 20. Our vector X = [10, 15, 20]. Then:\n",
    "\n",
    "l1(X) = 10 + 15 + 20 = 45\n",
    "\n",
    "l2(X) = sqr( 10^2 + 15^2 + 20^2) = sqr( 100 + 225 + 400) = **26.9**\n",
    "\n",
    "The euclidean distance is always smaller than the Manhattan distance.\n",
    "\n",
    "\n",
    "The normalized vector values are therefore:\n",
    "\n",
    "X_scaled_l1 = [ 10/45, 15/45, 20/45 ]      =  [0.22, 0.33, 0.44]\n",
    "\n",
    "X_scaled_l2 = [10/26.9, 15/26.9, 20/26.9 ] =  [0.37, 0.55, 0.74]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# dataset for the demo\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# the scaler - for robust scaling\n",
    "from sklearn.preprocessing import Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>MEDV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
       "4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  MEDV  \n",
       "0     15.3  396.90   4.98  24.0  \n",
       "1     17.8  396.90   9.14  21.6  \n",
       "2     17.8  392.83   4.03  34.7  \n",
       "3     18.7  394.63   2.94  33.4  \n",
       "4     18.7  396.90   5.33  36.2  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the the Boston House price data\n",
    "\n",
    "# this is how we load the boston dataset from sklearn\n",
    "boston_dataset = load_boston()\n",
    "\n",
    "# create a dataframe with the independent variables\n",
    "data = pd.DataFrame(boston_dataset.data,\n",
    "                      columns=boston_dataset.feature_names)\n",
    "\n",
    "# add target\n",
    "data['MEDV'] = boston_dataset.target\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((354, 13), (152, 13))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's separate into training and testing set\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.drop('MEDV', axis=1),\n",
    "                                                    data['MEDV'],\n",
    "                                                    test_size=0.3,\n",
    "                                                    random_state=0)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling to l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the scaler\n",
    "scaler = Normalizer(norm='l1') # for euclidean distance we change to norm='l2' \n",
    "\n",
    "# fit the scaler, this procedure does NOTHING\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# transform train and test sets\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1024.1,  744. , 1003.5,  858. ,  732.4,  859.3,  845.9,  847. ,\n",
       "        888.1,  826.7,  705.9,  808.9,  734. ,  784.6,  785.1,  819.6,\n",
       "       1212.9, 1258. ,  757.6, 1223. , 1236.1,  890.2, 1166.8, 1270. ,\n",
       "        919. ,  925.9, 1197. ,  770.3, 1118. ,  886.7,  719. ,  952. ,\n",
       "        933.4,  745. ,  805.6,  825.1,  678.5,  947.7,  910. ,  816. ,\n",
       "        769.1,  832.4,  763.5,  777. ,  850.4, 1137.7,  802.1,  703.4,\n",
       "        802.6,  688. ,  801.8,  877.3,  753.8,  912.4,  936.7,  813.6,\n",
       "       1194. ,  771.9,  739.5,  825.9,  890.8,  891.6,  844.4,  927.1,\n",
       "        998.6,  903.3, 1250.3,  739.5, 1095.3,  857.7,  834.8,  738.4,\n",
       "        855.1, 1238.4,  828.6,  958.4,  775. ,  923.4,  859.6,  830.7,\n",
       "        782.3,  796.8,  953.2,  857.9, 1249.9,  716.7,  803.9,  978.8,\n",
       "       1186.9,  904. , 1238.2,  981.6,  884.9,  924.9,  694.1,  725.2,\n",
       "        832.4,  717.7,  917. , 1263.9,  836.7,  813.5,  992.5,  823.4,\n",
       "       1224.3,  712.1,  841.2,  950.3,  702.6,  791.9,  830.4,  742.1,\n",
       "        793.8, 1110.5,  832.1, 1188.5,  837.4,  846.7,  809.5,  764.6,\n",
       "       1189. ,  770.4,  651.4,  717.8, 1265.8, 1004. ,  782.1,  922.7,\n",
       "        871.4,  860. ,  860.1,  766.5, 1259.6,  920.1,  708.2,  779.3,\n",
       "        785.7,  815. ,  825. ,  856.5,  784.2,  834.9, 1003.2,  963.2,\n",
       "        924.1, 1141.8, 1192.2,  777.5,  767.7, 1197.4,  887.2,  839.3,\n",
       "        819.9,  818.9,  822.7, 1006.2,  926.2,  748.9,  920.8,  790.7,\n",
       "        810.2, 1235.9,  889. ,  921.2,  895.3,  774.9,  924.8,  926. ,\n",
       "       1248.6, 1006.3,  817.5,  800.7, 1227.4,  831.4,  787.4, 1312. ,\n",
       "        832.1,  769.5, 1252.1,  822.7,  884. ,  787.2,  749.4,  827.8,\n",
       "        962.2,  702.8,  745.4,  731.7,  727.8,  743.7,  888.3,  720.2,\n",
       "       1332.3, 1185.2,  776.4,  784.6, 1255.2,  717.8, 1239. ,  782.1,\n",
       "        767.9,  807.6,  857.5, 1265.4,  737.9,  800. ,  769.3, 1231.7,\n",
       "        783.1,  667. ,  726.4,  758.5,  841.3, 1243.1, 1268.4, 1174.9,\n",
       "        736. ,  828.9,  827.5, 1270. , 1000.2,  919.4,  866. ,  681.7,\n",
       "       1249. ,  810.9,  901.7, 1218.9,  928.7,  744.7,  843.4,  736.1,\n",
       "        897.6,  804.6, 1219.5,  794.5,  830.4, 1241.1,  732. , 1211.2,\n",
       "        890.6,  838.1,  753.5,  795.8,  801.3, 1277.8, 1256. ,  801.3,\n",
       "        704.7,  808.2,  931.3,  977.4,  702.4,  713.7, 1240.1, 1001.4,\n",
       "        867.2,  806. ,  802.1,  774.1,  783.6,  739.2,  719.6, 1276.7,\n",
       "       1264.7,  865.4, 1104.8, 1162.6,  778.9,  781.8,  776.8,  945.3,\n",
       "       1234.4,  794.1,  830.3,  880.1,  788.6,  799.5, 1231.8, 1236.9,\n",
       "        669.1,  934.3, 1253.3, 1239.6,  734.5,  855.2,  753.4, 1260.9,\n",
       "        730.5, 1182.1, 1253. , 1004.4,  793.7,  847.5,  724.4, 1237.2,\n",
       "        941.6,  906.5,  859.6,  877.7,  842. ,  694.3,  997.8,  695.3,\n",
       "       1251.1, 1167. ,  837.6,  855. , 1301.8, 1238.6,  968.8,  822.6,\n",
       "       1216.7,  807.1,  775.9,  707.8, 1260.5,  943.2,  706.4,  839.5,\n",
       "        759.2, 1221.5,  923.8, 1219.9,  780.6,  755.1,  701.2,  757.2,\n",
       "        797.2,  731.4, 1237.1,  802.4, 1265.4,  785.5,  766.3,  839.7,\n",
       "       1255.2,  792.9, 1224.5,  748.3,  743.8,  790.3,  808.3,  760.4,\n",
       "        828.9,  854.6, 1226.7,  797. ,  788. ,  807.4,  900.5,  962.7,\n",
       "        770.2,  830.6])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's calculate the norm for each observation (feature vector)\n",
    "# original data\n",
    "\n",
    "np.round( np.linalg.norm(X_train, ord=1, axis=1), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's calculate the norm for each observation (feature vector)\n",
    "# scaled data\n",
    "\n",
    "np.round( np.linalg.norm(X_train_scaled, ord=1, axis=1), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling to l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the scaler\n",
    "scaler = Normalizer(norm='l2')\n",
    "\n",
    "# fit the scaler, this procedure does NOTHING\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# transform train and test sets\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "feml",
   "language": "python",
   "name": "feml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
